{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-Based News Summarization Project\n",
        "\n",
        "## Overview\n",
        "This Jupyter Notebook implements a Retrieval-Augmented Generation (RAG) system for summarizing news articles from the Hugging Face `cc_news` dataset. The project demonstrates expertise in Generative AI, vector embeddings, and evaluation metrics, suitable for showcasing on a resume.\n",
        "\n",
        "**Components**:\n",
        "- **Data Loading**: Fetches and preprocesses news articles.\n",
        "- **Vector Store**: Creates a FAISS index with Sentence Transformer embeddings.\n",
        "- **RAG Pipeline**: Retrieves relevant articles and generates summaries using BART.\n",
        "- **Evaluation**: Assesses retrieval (Precision@k, Recall@k, MRR) and generation (ROUGE) performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Install required dependencies and import libraries. Run this cell first to set up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install datasets==2.21.0 sentence-transformers==3.1.1 faiss-cpu==1.9.0 langchain==0.3.0 langchain_community==0.3.0 transformers==4.44.2 torch==2.4.1 pandas==2.2.3 numpy==1.26.4 rouge_score==0.1.2\n",
        "\n",
        "# import logging\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from datasets import load_dataset\n",
        "# from sentence_transformers import SentenceTransformer # pre-trained models for text embeddings\n",
        "# import faiss # for efficient similarity search\n",
        "# import pickle\n",
        "# from langchain.llms import HuggingFacePipeline # perform inference with HuggingFace models\n",
        "# from langchain.prompts import PromptTemplate # create prompts for LLMs\n",
        "# from langchain.chains import LLMChain # chain together LLM calls\n",
        "# from transformers import pipeline\n",
        "# from rouge_score import rouge_scorer\n",
        "# from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "# Setup\n",
        "# Install required dependencies and import libraries. Run this cell first.\n",
        "# Set OpenAI API key for GPT-3.5-Turbo (optional for BART).\n",
        "\n",
        "# !pip install datasets==2.21.0 sentence-transformers==3.1.1 faiss-cpu==1.9.0 langchain==0.3.0 langchain_community==0.3.0 langchain_openai==0.3.0 transformers==4.44.2 torch==2.4.1 pandas==2.2.3 numpy==1.26.4 rouge_score==0.1.2\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pickle\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set OpenAI API key (required for model_type='openai')\n",
        "# Option 1: Set environment variable (recommended)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"my_test_API_key\"\n",
        "# Option 2: Input key directly (less secure, for testing)\n",
        "try:\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        OPENAI_API_KEY = input(\"Enter your OpenAI API key (or press Enter to skip for BART): \")\n",
        "        if OPENAI_API_KEY:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "except Exception as e:\n",
        "    logger.warning(f\"OpenAI API key setup skipped: {e}\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "Fetch and preprocess news articles from the `cc_news` dataset on Hugging Face. This module loads a sample of articles and cleans the text for embedding and summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading dataset: cc_news\n",
            "INFO:__main__:Loaded 200 articles\n",
            "INFO:__main__:Preprocessing data\n",
            "INFO:__main__:Preprocessed 198 articles\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled 200 articles from the dataset\n",
            "Sample article title: ‘Indexpo’ in the city from June 16 to 18\n"
          ]
        }
      ],
      "source": [
        "class NewsDataLoader:\n",
        "    \"\"\"Loads and preprocesses news articles from Hugging Face dataset.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_name=\"cc_news\", sample_size=100):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.sample_size = sample_size\n",
        "        self.data = None\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Load dataset from Hugging Face and sample articles.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading dataset: {self.dataset_name}\")\n",
        "            dataset = load_dataset(self.dataset_name, split=\"train\")\n",
        "            self.data = dataset.to_pandas().sample(n=min(self.sample_size, len(dataset)), random_state=42)\n",
        "            print(f\"Sampled {len(self.data)} articles from the dataset\")\n",
        "            logger.info(f\"Loaded {len(self.data)} articles\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Clean and preprocess text data.\"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "        \n",
        "        logger.info(\"Preprocessing data\")\n",
        "        self.data = self.data.dropna(subset=['text'])\n",
        "        self.data['text'] = self.data['text'].str.strip().str.replace(r'\\s+', ' ', regex=True) # Remove extra whitespace, duplicate spaces\n",
        "        self.data = self.data[self.data['text'].str.len() > 100]\n",
        "        logger.info(f\"Preprocessed {len(self.data)} articles\")\n",
        "    \n",
        "    def get_articles(self):\n",
        "        \"\"\"Return preprocessed articles.\"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"Data not loaded or preprocessed.\")\n",
        "        return self.data[['title', 'text']].to_dict('records')\n",
        "\n",
        "# Load and preprocess data\n",
        "loader = NewsDataLoader(sample_size=200)\n",
        "loader.load_data()\n",
        "loader.preprocess_data()\n",
        "articles = loader.get_articles()\n",
        "print(f\"Sample article title: {articles[0]['title']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample article title: Urban and Small Stream Flood Advisory until 4:15PM CDT for western McHenry County and Boone County in Illinois\n",
            "Sample article text: × Urban and Small Stream Flood Advisory until 4:15PM CDT for western McHenry County and Boone County in Illinois * Urban and Small Stream Flood Advisory for… Western McHenry County in northeastern Illinois… Boone County in north central Illinois… * Until 415 PM CDT * At 110 PM CDT, Doppler radar indicated heavy rain due to thunderstorms. This will cause urban and small stream flooding in the advisory area. * Some locations that will experience flooding include… Belvidere, Woodstock, Harvard, Marengo, Poplar Grove, Capron, Hebron, Timberlane, Greenwood, Caledonia and Union. * Minor flooding was reported by law enforcement in Belvidere around 100 PM CDT.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sample article title: {articles[15]['title']}\")\n",
        "print(f\"Sample article text: {articles[15]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Store\n",
        "Create a FAISS vector store using Sentence Transformer embeddings for semantic search. This module enables efficient retrieval of relevant articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
            "/Users/user/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "INFO:__main__:Creating FAISS index\n",
            "Batches: 100%|██████████| 7/7 [00:01<00:00,  3.71it/s]\n",
            "INFO:__main__:Indexed 198 articles\n",
            "INFO:__main__:Saved index to faiss_index.bin and metadata to metadata.pkl\n",
            "INFO:__main__:Loaded index from faiss_index.bin and metadata from metadata.pkl\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search results: ['There are many youtuber’s just who make video', 'This Guy Goes To Hilariously Impressive Lengths To Take Instagrams Of His Girlfriend', 'Showcase your Zine', 'It Brits Celebrated Mrs. Alice x Misela at Annabel’s', 'Insomniac Magazine']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class VectorStore:\n",
        "    \"\"\"Manages creation and querying of FAISS vector store for news articles.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", index_path=\"faiss_index.bin\", metadata_path=\"metadata.pkl\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index_path = index_path\n",
        "        self.metadata_path = metadata_path\n",
        "        self.index = None\n",
        "        self.metadata = None\n",
        "    \n",
        "    def create_index(self, articles):\n",
        "        \"\"\"Create FAISS index from article texts.\"\"\"\n",
        "        \n",
        "        logger.info(\"Creating FAISS index\")\n",
        "        texts = [article['text'] for article in articles] # Extract text from articles\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True) # Generate embeddings for texts\n",
        "        dimension = embeddings.shape[1] # Get embedding dimension\n",
        "        self.index = faiss.IndexFlatL2(dimension) # Create FAISS index for L2 distance\n",
        "        self.index.add(embeddings.astype(np.float32)) # Add embeddings to FAISS index\n",
        "        self.metadata = articles # Store metadata for articles\n",
        "        logger.info(f\"Indexed {len(articles)} articles\")\n",
        "    \n",
        "    def save_index(self):\n",
        "        \"\"\"Save FAISS index and metadata to disk.\"\"\"\n",
        "\n",
        "        if self.index is None or self.metadata is None:\n",
        "            raise ValueError(\"Index or metadata not initialized.\")\n",
        "        faiss.write_index(self.index, self.index_path) # Save FAISS index to file\n",
        "        with open(self.metadata_path, 'wb') as f:\n",
        "            pickle.dump(self.metadata, f)\n",
        "        logger.info(f\"Saved index to {self.index_path} and metadata to {self.metadata_path}\")\n",
        "    \n",
        "    def load_index(self):\n",
        "        \"\"\"Load FAISS index and metadata from disk.\"\"\"\n",
        "        try:\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "            with open(self.metadata_path, 'rb') as f:\n",
        "                self.metadata = pickle.load(f)\n",
        "            logger.info(f\"Loaded index from {self.index_path} and metadata from {self.metadata_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading index: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def search(self, query, k=5):\n",
        "        \"\"\"Search for top-k relevant articles based on query.\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not loaded or created.\")\n",
        "        query_embedding = self.model.encode([query])[0]\n",
        "        distances, indices = self.index.search(np.array([query_embedding]).astype(np.float32), k)\n",
        "        results = [self.metadata[i] for i in indices[0]]\n",
        "        return results\n",
        "\n",
        "# Create and save vector store\n",
        "store = VectorStore()\n",
        "store.create_index(articles)\n",
        "store.save_index()\n",
        "store.load_index()\n",
        "results = store.search(\"Art\", k=5) # Search for articles related in vector store\n",
        "print(f\"Search results: {[article['title'] for article in results]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 75.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search results: ['Fox Paid $400 Million for 2018 World Cup Broadcast Rights, Then Team USA Got Eliminated', 'Let’s Talk About Paul Ryan’s Scorching Duplicity on Gun Violence, Mental Health, and, Well, Everything', 'Insomniac Magazine', \"In drug crisis hotbed, hoping for action on Trump's words\", 'Top US Military Officer Warns North Korea That US Military Ready']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = store.search(\"America\", k=5) # Search for articles related in vector store\n",
        "print(f\"Search results: {[article['title'] for article in results]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipeline\n",
        "Implement the RAG pipeline to retrieve relevant articles and generate summaries using a BART model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class RAGPipeline:\n",
        "#     \"\"\"Implements Retrieval-Augmented Generation for news summarization.\"\"\"\n",
        "    \n",
        "#     def __init__(self, vector_store, model_name=\"openai/gpt-3.5-turbo\"): # Hugging Face model for summarization, can be changed to any other model, example: \"t5-base\", \"openai/gpt-3.5-turbo\"\n",
        "#         self.vector_store = vector_store\n",
        "#         self.model_name = model_name\n",
        "#         self.llm = self._initialize_llm()\n",
        "    \n",
        "#     def _initialize_llm(self):\n",
        "#         \"\"\"Initialize Hugging Face model for summarization.\"\"\"\n",
        "#         logger.info(f\"Initializing LLM: {self.model_name}\")\n",
        "#         hf_pipeline = pipeline(\"summarization\", model=self.model_name, device=-1)\n",
        "#         llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "#         return llm\n",
        "    \n",
        "#     def create_prompt(self, query, retrieved_articles):\n",
        "#         \"\"\"Create prompt for summarization based on query and retrieved articles.\"\"\"\n",
        "#         context = \"\\n\".join([f\"Article: {article['text'][:1000]}\" for article in retrieved_articles]) # Limit to first 1000 characters for brevity\n",
        "#         template = \"\"\"\n",
        "#         Summarize the following news articles in 2-3 sentences, focusing on the topic: {query}.\n",
        "#         Context:\n",
        "#         {context}\n",
        "#         \"\"\"\n",
        "#         prompt = PromptTemplate(template=template, input_variables=[\"query\", \"context\"])\n",
        "#         return prompt.format(query=query, context=context)\n",
        "    \n",
        "#     def generate_summary(self, query, k=3):\n",
        "#         \"\"\"Generate summary for query using RAG.\"\"\"\n",
        "#         logger.info(f\"Generating summary for query: {query}\")\n",
        "#         retrieved_articles = self.vector_store.search(query, k=k) # Retrieve top-k articles\n",
        "#         if not retrieved_articles:\n",
        "#             logger.warning(\"No articles retrieved for the query.\")\n",
        "#             return {\"query\": query, \"summary\": \"No relevant articles found.\", \"retrieved_articles\": []}\n",
        "#         logger.info(f\"Retrieved {len(retrieved_articles)} articles for summarization\")\n",
        "\n",
        "#         # Create prompt and run summarization\n",
        "#         prompt = self.create_prompt(query, retrieved_articles)  \n",
        "#         chain = LLMChain(llm=self.llm, prompt=PromptTemplate(template=\"{text}\", input_variables=[\"text\"])) # Use LLMChain for summarization\n",
        "#         summary = chain.run(text=prompt)\n",
        "#         return {\n",
        "#             \"query\": query,\n",
        "#             \"summary\": summary,\n",
        "#             \"retrieved_articles\": [article['title'] for article in retrieved_articles]\n",
        "#         }\n",
        "\n",
        "# # Initialize RAG pipeline\n",
        "# rag = RAGPipeline(store)\n",
        "# result = rag.generate_summary(\"USA\", k=5)\n",
        "# print(f\"Summary: {result['summary']}\")\n",
        "# print(f\"Retrieved articles: {result['retrieved_articles']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Initializing LLM: bart\n",
            "/Users/user/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "INFO:__main__:Generating summary for query: USA\n",
            "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
            "INFO:__main__:Initializing LLM: openai\n",
            "INFO:__main__:Generating summary for query: USA\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BART Summary: Fox jumped into a bidding war with ESPN to win the English language broadcast rights for the 2018 World Cup. Fox topped ESPN in the bidding war by $200 million bringing its final costs to a whopping $400 million for the rights. The cost may have seemed like a good deal since the U.S. soccer team hadn’t missed a World Cup since 1986. However, now that Team USA was knocked out in the early stages of the tournament, it looks like a disastrous decision.\n",
            "BART Retrieved articles: ['Fox Paid $400 Million for 2018 World Cup Broadcast Rights, Then Team USA Got Eliminated', 'Top US Military Officer Warns North Korea That US Military Ready']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.433046 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.815848 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI test skipped: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ],
      "source": [
        "# RAG Pipeline\n",
        "# Implement the RAG pipeline to retrieve relevant articles and generate summaries using either OpenAI GPT-3.5-Turbo or Facebook BART.\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"Implements Retrieval-Augmented Generation for news summarization.\"\"\"\n",
        "    \n",
        "    def __init__(self, vector_store, model_type=\"bart\", openai_model=\"gpt-3.5-turbo\", bart_model=\"facebook/bart-large-cnn\"):\n",
        "        \"\"\"\n",
        "        Initialize RAG pipeline with specified model type.\n",
        "        Args:\n",
        "            vector_store (VectorStore): Instance for retrieval.\n",
        "            model_type (str): 'openai' for GPT-3.5-Turbo or 'bart' for BART.\n",
        "            openai_model (str): OpenAI model name (if model_type='openai').\n",
        "            bart_model (str): BART model name (if model_type='bart').\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.model_type = model_type.lower()\n",
        "        self.openai_model = openai_model\n",
        "        self.bart_model = bart_model\n",
        "        self.llm = self._initialize_llm()\n",
        "    \n",
        "    def _initialize_llm(self):\n",
        "        \"\"\"Initialize LLM based on model type.\"\"\"\n",
        "        logger.info(f\"Initializing LLM: {self.model_type}\")\n",
        "        try:\n",
        "            if self.model_type == \"openai\":\n",
        "                if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "                    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
        "                llm = ChatOpenAI(model_name=self.openai_model, temperature=0.7)\n",
        "            elif self.model_type == \"bart\":\n",
        "                hf_pipeline = pipeline(\"summarization\", model=self.bart_model, device=-1)\n",
        "                llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "            else:\n",
        "                raise ValueError(\"model_type must be 'openai' or 'bart'.\")\n",
        "            return llm\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing {self.model_type} model: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def create_prompt(self, query, retrieved_articles):\n",
        "        \"\"\"Create prompt for summarization based on query and retrieved articles.\"\"\"\n",
        "        context = \"\\n\".join([f\"Article: {article['text'][:1000]}\" for article in retrieved_articles])\n",
        "        if self.model_type == \"openai\":\n",
        "            template = \"\"\"\n",
        "            You are a helpful assistant tasked with summarizing news articles.\n",
        "            Summarize the following articles in 2-3 sentences, focusing on the topic: {query}.\n",
        "            Ensure the summary is concise, relevant, and factually accurate.\n",
        "            Context:\n",
        "            {context}\n",
        "            \"\"\"\n",
        "        else:  # BART\n",
        "            template = \"\"\"\n",
        "            Summarize the following news articles in 2-3 sentences, focusing on the topic: {query}.\n",
        "            Context:\n",
        "            {context}\n",
        "            \"\"\"\n",
        "        prompt = PromptTemplate(template=template, input_variables=[\"query\", \"context\"])\n",
        "        return prompt.format(query=query, context=context)\n",
        "    \n",
        "    def generate_summary(self, query, k=3):\n",
        "        \"\"\"Generate summary for query using RAG.\"\"\"\n",
        "        logger.info(f\"Generating summary for query: {query}\")\n",
        "        retrieved_articles = self.vector_store.search(query, k=k)\n",
        "        prompt = self.create_prompt(query, retrieved_articles)\n",
        "        chain = LLMChain(llm=self.llm, prompt=PromptTemplate(template=\"{text}\", input_variables=[\"text\"]))\n",
        "        summary = chain.run(text=prompt)\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"summary\": summary,\n",
        "            \"retrieved_articles\": [article['title'] for article in retrieved_articles]\n",
        "        }\n",
        "\n",
        "# Test RAG pipeline with both models\n",
        "# BART (default)\n",
        "rag_bart = RAGPipeline(store, model_type=\"bart\")\n",
        "result_bart = rag_bart.generate_summary(\"USA\", k=2)\n",
        "print(f\"BART Summary: {result_bart['summary']}\")\n",
        "print(f\"BART Retrieved articles: {result_bart['retrieved_articles']}\")\n",
        "\n",
        "# OpenAI \n",
        "try:\n",
        "    rag_openai = RAGPipeline(store, model_type=\"openai\")\n",
        "    result_openai = rag_openai.generate_summary(\"USA\", k=2)\n",
        "    print(f\"OpenAI Summary: {result_openai['summary']}\")\n",
        "    print(f\"OpenAI Retrieved articles: {result_openai['retrieved_articles']}\")\n",
        "except Exception as e:\n",
        "    print(f\"OpenAI test skipped: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "Evaluate the RAG system's retrieval (Precision@k, Recall@k, MRR) and generation (ROUGE) performance using a small evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.10it/s]\n",
            "INFO:__main__:Generating summary for query: UK\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.04it/s]\n",
            "INFO:__main__:Generating summary for query: USA\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics: {'precision@k': 0.16666666666666666, 'recall@k': 0.25, 'mrr': 0.25, 'rouge1': 0.026765188834154352, 'rouge2': 0.0, 'rougeL': 0.026765188834154352}\n"
          ]
        }
      ],
      "source": [
        "class Evaluator:\n",
        "    \"\"\"Evaluates retrieval and generation performance of the RAG system.\"\"\"\n",
        "    \n",
        "    def __init__(self, vector_store, rag_pipeline):\n",
        "        self.vector_store = vector_store\n",
        "        self.rag_pipeline = rag_pipeline\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    def evaluate_retrieval(self, queries: List[str], ground_truth: Dict[str, List[int]], k: int = 3) -> Dict[str, float]:\n",
        "        precisions, recalls, mrrs = [], [], []\n",
        "        \n",
        "        for query in queries:\n",
        "            retrieved_articles = self.vector_store.search(query, k=k)\n",
        "            retrieved_indices = [self.vector_store.metadata.index(article) for article in retrieved_articles]\n",
        "            relevant_indices = ground_truth.get(query, [])\n",
        "            \n",
        "            relevant_retrieved = len(set(retrieved_indices) & set(relevant_indices))\n",
        "            precision = relevant_retrieved / k if k > 0 else 0\n",
        "            precisions.append(precision)\n",
        "            \n",
        "            recall = relevant_retrieved / len(relevant_indices) if relevant_indices else 0\n",
        "            recalls.append(recall)\n",
        "            \n",
        "            mrr = 0\n",
        "            for rank, idx in enumerate(retrieved_indices, 1):\n",
        "                if idx in relevant_indices:\n",
        "                    mrr = 1 / rank\n",
        "                    break\n",
        "            mrrs.append(mrr)\n",
        "        \n",
        "        return {\n",
        "            \"precision@k\": np.mean(precisions),\n",
        "            \"recall@k\": np.mean(recalls),\n",
        "            \"mrr\": np.mean(mrrs)\n",
        "        }\n",
        "    \n",
        "    def evaluate_generation(self, queries: List[str], reference_summaries: Dict[str, str], k: int = 3) -> Dict[str, float]:\n",
        "        rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "        \n",
        "        for query in queries:\n",
        "            result = self.rag_pipeline.generate_summary(query, k=k)\n",
        "            generated_summary = result[\"summary\"]\n",
        "            reference_summary = reference_summaries.get(query, \"\")\n",
        "            \n",
        "            if reference_summary:\n",
        "                scores = self.rouge_scorer.score(reference_summary, generated_summary)\n",
        "                for metric in rouge_scores:\n",
        "                    rouge_scores[metric].append(scores[metric].fmeasure)\n",
        "        \n",
        "        return {\n",
        "            \"rouge1\": np.mean(rouge_scores[\"rouge1\"]) if rouge_scores[\"rouge1\"] else 0,\n",
        "            \"rouge2\": np.mean(rouge_scores[\"rouge2\"]) if rouge_scores[\"rouge2\"] else 0,\n",
        "            \"rougeL\": np.mean(rouge_scores[\"rougeL\"]) if rouge_scores[\"rougeL\"] else 0\n",
        "        }\n",
        "    \n",
        "    def evaluate_end_to_end(self, queries: List[str], ground_truth: Dict[str, List[int]], reference_summaries: Dict[str, str], k: int = 3) -> Dict[str, float]:\n",
        "        retrieval_metrics = self.evaluate_retrieval(queries, ground_truth, k)\n",
        "        generation_metrics = self.evaluate_generation(queries, reference_summaries, k)\n",
        "        return {**retrieval_metrics, **generation_metrics}\n",
        "\n",
        "# Setup evaluation data (examples)\n",
        "queries = [\"UK\", \"USA\"]\n",
        "ground_truth = {\n",
        "    \"UK\": [9, 13],  \n",
        "    \"USA\": [4, 19]\n",
        "}\n",
        "reference_summaries = {\n",
        "    \"UK\": \"Ross Kemp reacts BRILLIANTLY to England's win over Colombia\",\n",
        "    \"USA\": \"Donald Trump can't hide behind patriotism if he won't condemn neo-Nazi thugs\"\n",
        "}\n",
        "\n",
        "# Run evaluation\n",
        "evaluator = Evaluator(store, rag_bart)\n",
        "metrics = evaluator.evaluate_end_to_end(queries, ground_truth, reference_summaries, k=3)\n",
        "print(\"Evaluation Metrics:\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretations\n",
        "- **Precision@k = 0.167 (16.7%)**:\n",
        "  - Only 16.7% of the top-3 retrieved articles are relevant to the query.\n",
        "  - Poor retrieval accuracy; many retrieved articles are irrelevant (limited dataset use can be the reason).\n",
        "\n",
        "- **Recall@k = 0.25 (25%)**:\n",
        "  - *25% of all relevant articles are retrieved in the top-3.\n",
        "  -  Misses most relevant articles, likely due to small dataset.\n",
        "\n",
        "- **Mean Reciprocal Rank (MRR) = 0.25**:\n",
        "  - First relevant article appears around rank 4 on average (1/0.25).\n",
        "  - Relevant articles are ranked low, reducing retrieval effectiveness.\n",
        "\n",
        "- **ROUGE-1 = 0.027 (2.7%)**:\n",
        "  - 2.7% overlap of single words between generated and reference summaries (also likely due to small dataset).\n",
        "  - Minimal content similarity, possibly due to irrelevant retrieved articles or paraphrasing.\n",
        "\n",
        "- **ROUGE-2 = 0.0 (0%)**:\n",
        "  - No overlap of two-word phrases between generated and reference summaries.\n",
        "  - No shared phrases, indicating highly abstractive summaries or mismatched references.\n",
        "\n",
        "- **ROUGE-L = 0.027 (2.7%)**:\n",
        "  - 2.7% overlap in longest common subsequence, measuring structural similarity.\n",
        "  - Poor structural alignment, likely due to same issues as ROUGE-1.\n",
        "\n",
        "- **Overall**:\n",
        "  - **Retrieval**: Weak performance (low precision, recall, MRR) suggests issues with embeddings or dataset size.\n",
        "  - **Generation**: Very low ROUGE scores indicate summaries don’t match references, likely due to poor retrieval or reference quality.\n",
        "  - **Next Steps**: Increase the dataset size, refine the `ground_truth` and the reference summaries, maybe also upgrade the embedding model (e.g., `all-mpnet-base-v2`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Exploration\n",
        "Test the RAG system by entering custom queries. Run this cell to try different topics and inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Generating summary for query: Donald Trump\n",
            "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it]\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.382580 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.920185 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieved articles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mretrieved_articles\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43minteractive_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36minteractive_query\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.lower() == \u001b[33m'\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mrag_openai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieved articles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mretrieved_articles\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mRAGPipeline.generate_summary\u001b[39m\u001b[34m(self, query, k)\u001b[39m\n\u001b[32m     64\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.create_prompt(query, retrieved_articles)\n\u001b[32m     65\u001b[39m chain = LLMChain(llm=\u001b[38;5;28mself\u001b[39m.llm, prompt=PromptTemplate(template=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[33m\"\u001b[39m, input_variables=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m summary = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: summary,\n\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretrieved_articles\u001b[39m\u001b[33m\"\u001b[39m: [article[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m retrieved_articles]\n\u001b[32m     71\u001b[39m }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/base.py:611\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[32m0\u001b[39m], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    607\u001b[39m         _output_key\n\u001b[32m    608\u001b[39m     ]\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    612\u001b[39m         _output_key\n\u001b[32m    613\u001b[39m     ]\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    617\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but none were provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/llm.py:126\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    123\u001b[39m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    124\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain/chains/llm.py:138\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m         cast(List, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    147\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:781\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GitHub/GenAI_RAG_News_Summary/venv/lib/python3.11/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "def interactive_query():\n",
        "    \"\"\"Run interactive query loop for testing the RAG system.\"\"\"\n",
        "    while True:\n",
        "        query = input(\"Enter a query (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        result = rag_openai.generate_summary(query, k=3)\n",
        "        print(f\"\\nSummary: {result['summary']}\")\n",
        "        print(f\"Retrieved articles: {result['retrieved_articles']}\")\n",
        "\n",
        "interactive_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Articles\n",
        "Display a sample of article titles and their texts to help create evaluation data (e.g., ground truth indices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 0: ‘Indexpo’ in the city from June 16 to 18\n",
            "Text (first 200 chars): Nashik : Indore Infoline Pvt. Ltd has organised an industrial expo ‘Indexpo’ at Thakkar’s Dome, ABB Circle, Nashik from June 16 to 18, informed managing director Rajkumar Agrawal in a media briefing y\n",
            "\n",
            "Index 1: Quit notice: We are monitoring situation in Northern Nigeria - South-East Governors\n",
            "Text (first 200 chars): South-East Governors on Monday re-assured Ndigbo residing in different parts of the country of their safety. The Governors, who met in Enugu said they were in constant touch with the Northern Governor\n",
            "\n",
            "Index 2: Two teens charged, accused of leading police on chase in reporte - | WBTV Charlotte\n",
            "Text (first 200 chars): The two teenagers that were arrested in connection to a vehicle pursuit involving a stolen vehicle Wednesday have been identified. Charlotte-Mecklenburg Police identified the two teens as 17-year-old \n",
            "\n",
            "Index 3: Golf: 'Green Mile' strewn with PGA victims at Quail Hollow\n",
            "Text (first 200 chars): CHARLOTTE, North Carolina (Reuters) - Jason Day began the day in the final threesome, two strokes off the lead, and had the usual ups and downs on Saturday before his quadruple-bogey at the Quail Holl\n",
            "\n",
            "Index 4: Donald Trump can't hide behind patriotism if he won't condemn neo-Nazi thugs\n",
            "Text (first 200 chars): Donald Trump’s baser instincts served him well during the campaign last year. Why expect him to change now? We know how he works. If the mainstream media says one thing, then he says the other. If the\n",
            "\n",
            "Index 5: Catalonia Demands Independence from Spain\n",
            "Text (first 200 chars): The region of Catalonia is trying to claim its independence from Spain, and recent protests have turned violent. In the weeks leading up to October 1, the central government in Spain tried almost ever\n",
            "\n",
            "Index 6: Maine Mendoza gives heartfelt birthday message for Alden Richards\n",
            "Text (first 200 chars): KAPUSO actor and host Alden Richards celebrated his 25th birthday in yesterday’s episode of the noontime show “Eat Bulaga.” For his birthday production number, Alden performed a dance number with “fly\n",
            "\n",
            "Index 7: Global Sports Turf Seed Market 2018- Hancock Seed, Pennington Seed, The Scotts Company, Barenbrug Group, Turf Merchants\n",
            "Text (first 200 chars): Index Markets Research offers a detailed research on Global Sports Turf Seed Market with the future prospects of the market to provide the current state and emerging trends in the market. The report c\n",
            "\n",
            "Index 8: Insomniac Magazine\n",
            "Text (first 200 chars): Since its initial inception in 1995, as Insomniac “The World’s Only Hip Hop Industry Publication”™ has progressed from focusing on the business of Hip Hop to also covering the entertainment and media \n",
            "\n",
            "Index 9: Ross Kemp reacts BRILLIANTLY to England's win over Colombia\n",
            "Text (first 200 chars): ENGLAND’S victory over Colombia last night was “everything” for TV hard man Ross Kemp. Gareth Southgate's side put in a gutsy performance as they saw off the South American side in Moscow to progress \n",
            "\n",
            "Index 10: Judge: More parking for proposed NJ mosque unconstitutional\n",
            "Text (first 200 chars): 3:03 Talkback: Mark Young talks about Vice Mayor Gene Gallo's tragic loss of his wife Pause 2:15 The zany 9th Annual Shamrock Shiver Charity Plunge 1:17 Samoset Neighborhood Association looks to 2017 \n",
            "\n",
            "Index 11: Relievers fail to stop Diamondbacks\n",
            "Text (first 200 chars): LOS ANGELES >> Dodgers relievers are reporting for work early and often again. For the most part, the relief corps has been up to the early-season workload. But Chris Hatcher gave up a leadoff home ru\n",
            "\n",
            "Index 12: Insta360 Will Soon Allow 8K Playback, Even on Smartphones\n",
            "Text (first 200 chars): Share Viewing immersive content hasn’t yet caught up to the increasing resolution of 360 cameras — but 360 camera manufacturer Insta360 has developed a solution that will allow viewers to watch 8K imm\n",
            "\n",
            "Index 13: Missing girls from Buckingham found safe and well\n",
            "Text (first 200 chars): Police appealing for help to find two missing girls from the Buckingham area have thanked the public for their help after the girls were found safe. Kelsey Cooney and Summer Jones, both aged 14 went m\n",
            "\n",
            "Index 14: BRIEF-Oasis Petroleum Announces Public Offering Of 32 Mln Shares Of Common Stock\n",
            "Text (first 200 chars): Dec 11 (Reuters) - Oasis Petroleum Inc: * . ANNOUNCES PUBLIC OFFERING OF COMMON STOCK * SAYS OFFERING 32.0 MILLION COMMON SHARES * - TO USE PROCEEDS FROM OFFERING TO FUND A PORTION OF ACQUISITION OF A\n",
            "\n",
            "Index 15: Urban and Small Stream Flood Advisory until 4:15PM CDT for western McHenry County and Boone County in Illinois\n",
            "Text (first 200 chars): × Urban and Small Stream Flood Advisory until 4:15PM CDT for western McHenry County and Boone County in Illinois * Urban and Small Stream Flood Advisory for… Western McHenry County in northeastern Ill\n",
            "\n",
            "Index 16: तालाब में डूबने से बालिका की मौत\n",
            "Text (first 200 chars): Read the latest and breaking Hindi news on amarujala.com. Get live Hindi news about India and the World from politics, sports, bollywood, business, cities, lifestyle, astrology, spirituality, jobs and\n",
            "\n",
            "Index 17: Top Tory throws weight behind EBacc changes\n",
            "Text (first 200 chars): There is a case for including skills such as engineering in the English Baccalaureate, the chair of the Education Select Committee has said. Neil Carmichael, chair of the Commons committee, has come o\n",
            "\n",
            "Index 18: New DNA structure resembling a ‘twisted knot’ discovered in living human cells\n",
            "Text (first 200 chars): Scientists have identified a new DNA structure inside living human cells that looks like a twisted “knot”. Called the i-motif, the DNA form has been described as “a four-stranded knot of DNA” and is d\n",
            "\n",
            "Index 19: Top US Military Officer Warns North Korea That US Military Ready\n",
            "Text (first 200 chars): (AP) – The top U.S. military officer is warning during a trip to Seoul that the United States is ready to use the “full range” of its military capabilities to defend itself and its allies from North K\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display first 5 articles for inspection\n",
        "for i, article in enumerate(articles[:20]):\n",
        "    print(f\"Index {i}: {article['title']}\")\n",
        "    print(f\"Text (first 200 chars): {article['text'][:200]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- **Evaluation Data**: The `ground_truth` and `reference_summaries` are placeholders. Inspect articles using the visualization cell above to assign correct indices and write reference summaries.\n",
        "- **Performance**: The `facebook/bart-large-cnn` model is CPU-intensive. For faster execution, one can try `distilbart-cnn-6-6` or use a GPU with `torch` CUDA support.\n",
        "- **Future Extensions**:\n",
        "  - Adding a web UI with Streamlit for better interactivity.\n",
        "  - Using a larger dataset or CNN/DailyMail for pre-annotated summaries.\n",
        "  - Implementing additional metrics like BLEU or human evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
